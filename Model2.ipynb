{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow -q\n",
    "%pip install pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras import models, layers, activations,\\\n",
    "optimizers, utils, losses, initializers, metrics, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "batch_size = 120\n",
    "patience = 48\n",
    "learning_rate = 1e-4\n",
    "model_path = 'checkpoints/model.keras'\n",
    "train_path = 'train'\n",
    "exists = os.path.exists(model_path)\n",
    "items = len(os.listdir(train_path))\n",
    "classes = 1 if items < 3 else items\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing (Resizing)         (None, 120, 120, 3)       0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 120, 120, 3)       0         \n",
      "                                                                 \n",
      " random_rotation (RandomRot  (None, 120, 120, 3)       0         \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 120, 120, 3)       0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 120, 120, 3)       12        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 114, 114, 40)      5920      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 57, 57, 40)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 53, 53, 60)        60060     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 26, 26, 60)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 80)        43280     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 12, 12, 80)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 11520)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 11520)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 102)               1175142   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 102)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10300     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 102)               10302     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 102)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10300     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 102)               10302     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 102)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10300     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 68)                6868      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 17)                1173      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1343959 (5.13 MB)\n",
      "Trainable params: 1343953 (5.13 MB)\n",
      "Non-trainable params: 6 (24.00 Byte)\n",
      "_________________________________________________________________\n",
      "Found 1800 files belonging to 17 classes.\n",
      "Using 1440 files for training.\n",
      "Found 1800 files belonging to 17 classes.\n",
      "Using 360 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Carrega modelo se já existir um checkpoint, caso contrário, o cria.\n",
    "model = models.load_model(model_path) \\\n",
    "if exists \\\n",
    "else models.Sequential([\n",
    "  layers.Resizing(120, 120),\n",
    "  layers.Rescaling(1.0/255),\n",
    "  layers.RandomRotation((-0.1, 0.1)),\n",
    "  layers.RandomFlip(),\n",
    "  layers.BatchNormalization(),\n",
    "  \n",
    "  layers.Conv2D(40, (7, 7),\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  \n",
    "  layers.Conv2D(60, (5, 5),\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  \n",
    "  layers.Conv2D(80, (3, 3),\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  \n",
    "  layers.Flatten(),\n",
    "  \n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(102,\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  \n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(100,\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  \n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(102,\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  \n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(100,\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  \n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(102,\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  \n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(100,\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  \n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(68,\n",
    "    activation = 'relu',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  ),\n",
    "  \n",
    "  layers.Dense(classes,\n",
    "    activation = 'sigmoid',\n",
    "    kernel_initializer = initializers.RandomUniform()\n",
    "  )\n",
    "])\n",
    "print(exists)\n",
    "if not exists:\n",
    "  model.compile(\n",
    "    optimizer = optimizers.Adam(\n",
    "      learning_rate = learning_rate\n",
    "    ),\n",
    "    loss = losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [ 'accuracy' ]\n",
    "  )\n",
    "else:\n",
    "  model.summary()\n",
    "  \n",
    "train = utils.image_dataset_from_directory(\n",
    "  train_path,\n",
    "  validation_split= 0.2,\n",
    "  subset= \"training\",\n",
    "  seed= 123,\n",
    "  shuffle= True,\n",
    "  image_size= (224, 224),\n",
    "  batch_size= batch_size\n",
    ")\n",
    "\n",
    "test = utils.image_dataset_from_directory(\n",
    "  train_path,\n",
    "  validation_split= 0.2,\n",
    "  subset= \"validation\",\n",
    "  seed= 123,\n",
    "  shuffle= True,\n",
    "  image_size= (224, 224),\n",
    "  batch_size= batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\disrct\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "12/12 [==============================] - 12s 818ms/step - loss: 2.8276 - accuracy: 0.1097 - val_loss: 2.8035 - val_accuracy: 0.1167 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "12/12 [==============================] - 11s 863ms/step - loss: 2.8054 - accuracy: 0.1097 - val_loss: 2.8009 - val_accuracy: 0.1167 - lr: 9.9999e-04\n",
      "Epoch 3/10000\n",
      "12/12 [==============================] - 11s 861ms/step - loss: 2.8205 - accuracy: 0.1111 - val_loss: 2.8257 - val_accuracy: 0.1194 - lr: 9.9998e-04\n",
      "Epoch 4/10000\n",
      "12/12 [==============================] - 11s 856ms/step - loss: 2.8047 - accuracy: 0.1118 - val_loss: 2.7884 - val_accuracy: 0.1333 - lr: 9.9997e-04\n",
      "Epoch 5/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 2.7693 - accuracy: 0.1257 - val_loss: 2.7095 - val_accuracy: 0.1361 - lr: 9.9996e-04\n",
      "Epoch 6/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 2.7186 - accuracy: 0.1361 - val_loss: 2.7232 - val_accuracy: 0.1417 - lr: 9.9995e-04\n",
      "Epoch 7/10000\n",
      "12/12 [==============================] - 11s 854ms/step - loss: 2.7274 - accuracy: 0.1319 - val_loss: 2.7350 - val_accuracy: 0.1278 - lr: 9.9994e-04\n",
      "Epoch 8/10000\n",
      "12/12 [==============================] - 11s 860ms/step - loss: 2.7186 - accuracy: 0.1333 - val_loss: 2.6731 - val_accuracy: 0.1278 - lr: 9.9993e-04\n",
      "Epoch 9/10000\n",
      "12/12 [==============================] - 11s 864ms/step - loss: 2.6871 - accuracy: 0.1437 - val_loss: 2.6658 - val_accuracy: 0.1556 - lr: 9.9992e-04\n",
      "Epoch 10/10000\n",
      "12/12 [==============================] - 10s 855ms/step - loss: 2.6842 - accuracy: 0.1410 - val_loss: 2.6676 - val_accuracy: 0.1472 - lr: 9.9991e-04\n",
      "Epoch 11/10000\n",
      "12/12 [==============================] - 11s 933ms/step - loss: 2.6748 - accuracy: 0.1396 - val_loss: 2.6332 - val_accuracy: 0.1611 - lr: 9.9989e-04\n",
      "Epoch 12/10000\n",
      "12/12 [==============================] - 10s 842ms/step - loss: 2.6578 - accuracy: 0.1431 - val_loss: 2.6096 - val_accuracy: 0.1333 - lr: 9.9988e-04\n",
      "Epoch 13/10000\n",
      "12/12 [==============================] - 11s 860ms/step - loss: 2.6330 - accuracy: 0.1410 - val_loss: 2.5757 - val_accuracy: 0.1444 - lr: 9.9987e-04\n",
      "Epoch 14/10000\n",
      "12/12 [==============================] - 11s 869ms/step - loss: 2.6024 - accuracy: 0.1361 - val_loss: 2.5488 - val_accuracy: 0.1417 - lr: 9.9986e-04\n",
      "Epoch 15/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 2.5895 - accuracy: 0.1590 - val_loss: 2.4916 - val_accuracy: 0.1806 - lr: 9.9985e-04\n",
      "Epoch 16/10000\n",
      "12/12 [==============================] - 11s 862ms/step - loss: 2.5420 - accuracy: 0.1507 - val_loss: 2.4890 - val_accuracy: 0.1306 - lr: 9.9984e-04\n",
      "Epoch 17/10000\n",
      "12/12 [==============================] - 11s 876ms/step - loss: 2.5275 - accuracy: 0.1528 - val_loss: 2.4492 - val_accuracy: 0.1611 - lr: 9.9983e-04\n",
      "Epoch 18/10000\n",
      "12/12 [==============================] - 10s 851ms/step - loss: 2.4832 - accuracy: 0.1500 - val_loss: 2.3895 - val_accuracy: 0.1500 - lr: 9.9982e-04\n",
      "Epoch 19/10000\n",
      "12/12 [==============================] - 11s 870ms/step - loss: 2.4731 - accuracy: 0.1451 - val_loss: 2.3944 - val_accuracy: 0.1500 - lr: 9.9981e-04\n",
      "Epoch 20/10000\n",
      "12/12 [==============================] - 10s 839ms/step - loss: 2.4598 - accuracy: 0.1486 - val_loss: 2.3624 - val_accuracy: 0.1611 - lr: 9.9980e-04\n",
      "Epoch 21/10000\n",
      "12/12 [==============================] - 11s 867ms/step - loss: 2.4533 - accuracy: 0.1535 - val_loss: 2.3416 - val_accuracy: 0.1611 - lr: 9.9979e-04\n",
      "Epoch 22/10000\n",
      "12/12 [==============================] - 11s 878ms/step - loss: 2.4100 - accuracy: 0.1611 - val_loss: 2.3564 - val_accuracy: 0.1444 - lr: 9.9978e-04\n",
      "Epoch 23/10000\n",
      "12/12 [==============================] - 11s 862ms/step - loss: 2.4002 - accuracy: 0.1667 - val_loss: 2.3069 - val_accuracy: 0.1556 - lr: 9.9977e-04\n",
      "Epoch 24/10000\n",
      "12/12 [==============================] - 11s 862ms/step - loss: 2.4040 - accuracy: 0.1569 - val_loss: 2.3135 - val_accuracy: 0.1639 - lr: 9.9976e-04\n",
      "Epoch 25/10000\n",
      "12/12 [==============================] - 10s 847ms/step - loss: 2.3861 - accuracy: 0.1660 - val_loss: 2.3544 - val_accuracy: 0.1778 - lr: 9.9975e-04\n",
      "Epoch 26/10000\n",
      "12/12 [==============================] - 10s 848ms/step - loss: 2.4005 - accuracy: 0.1597 - val_loss: 2.2691 - val_accuracy: 0.1528 - lr: 9.9974e-04\n",
      "Epoch 27/10000\n",
      "12/12 [==============================] - 10s 849ms/step - loss: 2.3545 - accuracy: 0.1667 - val_loss: 2.2688 - val_accuracy: 0.1694 - lr: 9.9973e-04\n",
      "Epoch 28/10000\n",
      "12/12 [==============================] - 11s 866ms/step - loss: 2.3419 - accuracy: 0.1667 - val_loss: 2.2516 - val_accuracy: 0.1639 - lr: 9.9972e-04\n",
      "Epoch 29/10000\n",
      "12/12 [==============================] - 10s 842ms/step - loss: 2.3564 - accuracy: 0.1681 - val_loss: 2.3052 - val_accuracy: 0.1806 - lr: 9.9971e-04\n",
      "Epoch 30/10000\n",
      "12/12 [==============================] - 11s 865ms/step - loss: 2.3786 - accuracy: 0.1639 - val_loss: 2.2664 - val_accuracy: 0.1611 - lr: 9.9969e-04\n",
      "Epoch 31/10000\n",
      "12/12 [==============================] - 10s 842ms/step - loss: 2.3242 - accuracy: 0.1750 - val_loss: 2.2519 - val_accuracy: 0.1611 - lr: 9.9968e-04\n",
      "Epoch 32/10000\n",
      "12/12 [==============================] - 11s 860ms/step - loss: 2.3415 - accuracy: 0.1625 - val_loss: 2.2603 - val_accuracy: 0.1556 - lr: 9.9967e-04\n",
      "Epoch 33/10000\n",
      "12/12 [==============================] - 10s 854ms/step - loss: 2.3128 - accuracy: 0.1757 - val_loss: 2.2982 - val_accuracy: 0.1528 - lr: 9.9966e-04\n",
      "Epoch 34/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 2.3163 - accuracy: 0.1750 - val_loss: 2.2297 - val_accuracy: 0.1639 - lr: 9.9965e-04\n",
      "Epoch 35/10000\n",
      "12/12 [==============================] - 10s 852ms/step - loss: 2.3100 - accuracy: 0.1806 - val_loss: 2.2436 - val_accuracy: 0.1722 - lr: 9.9964e-04\n",
      "Epoch 36/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 2.2872 - accuracy: 0.1778 - val_loss: 2.2135 - val_accuracy: 0.1722 - lr: 9.9963e-04\n",
      "Epoch 37/10000\n",
      "12/12 [==============================] - 11s 868ms/step - loss: 2.2854 - accuracy: 0.1757 - val_loss: 2.2365 - val_accuracy: 0.1722 - lr: 9.9962e-04\n",
      "Epoch 38/10000\n",
      "12/12 [==============================] - 11s 862ms/step - loss: 2.2789 - accuracy: 0.1826 - val_loss: 2.2292 - val_accuracy: 0.1667 - lr: 9.9961e-04\n",
      "Epoch 39/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 2.3299 - accuracy: 0.1819 - val_loss: 2.2240 - val_accuracy: 0.1611 - lr: 9.9960e-04\n",
      "Epoch 40/10000\n",
      "12/12 [==============================] - 11s 915ms/step - loss: 2.2771 - accuracy: 0.1819 - val_loss: 2.2093 - val_accuracy: 0.2056 - lr: 9.9959e-04\n",
      "Epoch 41/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 2.2745 - accuracy: 0.1792 - val_loss: 2.1930 - val_accuracy: 0.1694 - lr: 9.9958e-04\n",
      "Epoch 42/10000\n",
      "12/12 [==============================] - 11s 883ms/step - loss: 2.2583 - accuracy: 0.1722 - val_loss: 2.2325 - val_accuracy: 0.1722 - lr: 9.9957e-04\n",
      "Epoch 43/10000\n",
      "12/12 [==============================] - 11s 894ms/step - loss: 2.2283 - accuracy: 0.1903 - val_loss: 2.2006 - val_accuracy: 0.1778 - lr: 9.9956e-04\n",
      "Epoch 44/10000\n",
      "12/12 [==============================] - 10s 848ms/step - loss: 2.2285 - accuracy: 0.1826 - val_loss: 2.1796 - val_accuracy: 0.1972 - lr: 9.9955e-04\n",
      "Epoch 45/10000\n",
      "12/12 [==============================] - 11s 862ms/step - loss: 2.2220 - accuracy: 0.1847 - val_loss: 2.1598 - val_accuracy: 0.1889 - lr: 9.9954e-04\n",
      "Epoch 46/10000\n",
      "12/12 [==============================] - 11s 885ms/step - loss: 2.2204 - accuracy: 0.1847 - val_loss: 2.1889 - val_accuracy: 0.2056 - lr: 9.9953e-04\n",
      "Epoch 47/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 2.1776 - accuracy: 0.2062 - val_loss: 2.1207 - val_accuracy: 0.2000 - lr: 9.9952e-04\n",
      "Epoch 48/10000\n",
      "12/12 [==============================] - 11s 852ms/step - loss: 2.1708 - accuracy: 0.2000 - val_loss: 2.1375 - val_accuracy: 0.1889 - lr: 9.9950e-04\n",
      "Epoch 49/10000\n",
      "12/12 [==============================] - 11s 876ms/step - loss: 2.1737 - accuracy: 0.1903 - val_loss: 2.0975 - val_accuracy: 0.1889 - lr: 9.9949e-04\n",
      "Epoch 50/10000\n",
      "12/12 [==============================] - 10s 830ms/step - loss: 2.1770 - accuracy: 0.2028 - val_loss: 2.1114 - val_accuracy: 0.1944 - lr: 9.9948e-04\n",
      "Epoch 51/10000\n",
      "12/12 [==============================] - 11s 878ms/step - loss: 2.1649 - accuracy: 0.1924 - val_loss: 2.1196 - val_accuracy: 0.2111 - lr: 9.9947e-04\n",
      "Epoch 52/10000\n",
      "12/12 [==============================] - 10s 852ms/step - loss: 2.1677 - accuracy: 0.1903 - val_loss: 2.1088 - val_accuracy: 0.1889 - lr: 9.9946e-04\n",
      "Epoch 53/10000\n",
      "12/12 [==============================] - 11s 884ms/step - loss: 2.1615 - accuracy: 0.1958 - val_loss: 2.1656 - val_accuracy: 0.1833 - lr: 9.9945e-04\n",
      "Epoch 54/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 2.1323 - accuracy: 0.1833 - val_loss: 2.1537 - val_accuracy: 0.1917 - lr: 9.9944e-04\n",
      "Epoch 55/10000\n",
      "12/12 [==============================] - 11s 875ms/step - loss: 2.1414 - accuracy: 0.1979 - val_loss: 2.0681 - val_accuracy: 0.2306 - lr: 9.9943e-04\n",
      "Epoch 56/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 2.1190 - accuracy: 0.2104 - val_loss: 2.0642 - val_accuracy: 0.2083 - lr: 9.9942e-04\n",
      "Epoch 57/10000\n",
      "12/12 [==============================] - 11s 890ms/step - loss: 2.0760 - accuracy: 0.2139 - val_loss: 2.0578 - val_accuracy: 0.1861 - lr: 9.9941e-04\n",
      "Epoch 58/10000\n",
      "12/12 [==============================] - 11s 864ms/step - loss: 2.0694 - accuracy: 0.2125 - val_loss: 2.0547 - val_accuracy: 0.2056 - lr: 9.9940e-04\n",
      "Epoch 59/10000\n",
      "12/12 [==============================] - 11s 895ms/step - loss: 2.0762 - accuracy: 0.2201 - val_loss: 2.0186 - val_accuracy: 0.2333 - lr: 9.9939e-04\n",
      "Epoch 60/10000\n",
      "12/12 [==============================] - 11s 879ms/step - loss: 2.0510 - accuracy: 0.2264 - val_loss: 2.0019 - val_accuracy: 0.2250 - lr: 9.9938e-04\n",
      "Epoch 61/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 2.0639 - accuracy: 0.2153 - val_loss: 2.0101 - val_accuracy: 0.2444 - lr: 9.9937e-04\n",
      "Epoch 62/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 2.0346 - accuracy: 0.2181 - val_loss: 2.0097 - val_accuracy: 0.2222 - lr: 9.9936e-04\n",
      "Epoch 63/10000\n",
      "12/12 [==============================] - 11s 851ms/step - loss: 2.0519 - accuracy: 0.2167 - val_loss: 2.0303 - val_accuracy: 0.2306 - lr: 9.9935e-04\n",
      "Epoch 64/10000\n",
      "12/12 [==============================] - 10s 851ms/step - loss: 2.0591 - accuracy: 0.2382 - val_loss: 2.0565 - val_accuracy: 0.2167 - lr: 9.9934e-04\n",
      "Epoch 65/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 2.0687 - accuracy: 0.2083 - val_loss: 1.9960 - val_accuracy: 0.2694 - lr: 9.9933e-04\n",
      "Epoch 66/10000\n",
      "12/12 [==============================] - 11s 874ms/step - loss: 2.0391 - accuracy: 0.2417 - val_loss: 1.9503 - val_accuracy: 0.2528 - lr: 9.9932e-04\n",
      "Epoch 67/10000\n",
      "12/12 [==============================] - 11s 880ms/step - loss: 1.9730 - accuracy: 0.2528 - val_loss: 1.9603 - val_accuracy: 0.2472 - lr: 9.9930e-04\n",
      "Epoch 68/10000\n",
      "12/12 [==============================] - 11s 880ms/step - loss: 1.9899 - accuracy: 0.2521 - val_loss: 1.9380 - val_accuracy: 0.2639 - lr: 9.9929e-04\n",
      "Epoch 69/10000\n",
      "12/12 [==============================] - 11s 921ms/step - loss: 1.9644 - accuracy: 0.2500 - val_loss: 1.9359 - val_accuracy: 0.2778 - lr: 9.9928e-04\n",
      "Epoch 70/10000\n",
      "12/12 [==============================] - 11s 864ms/step - loss: 1.9907 - accuracy: 0.2361 - val_loss: 1.9385 - val_accuracy: 0.2361 - lr: 9.9927e-04\n",
      "Epoch 71/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.9612 - accuracy: 0.2507 - val_loss: 1.8877 - val_accuracy: 0.2417 - lr: 9.9926e-04\n",
      "Epoch 72/10000\n",
      "12/12 [==============================] - 11s 875ms/step - loss: 1.9307 - accuracy: 0.2722 - val_loss: 1.9304 - val_accuracy: 0.2583 - lr: 9.9925e-04\n",
      "Epoch 73/10000\n",
      "12/12 [==============================] - 10s 847ms/step - loss: 1.9425 - accuracy: 0.2576 - val_loss: 1.9162 - val_accuracy: 0.3111 - lr: 9.9924e-04\n",
      "Epoch 74/10000\n",
      "12/12 [==============================] - 11s 860ms/step - loss: 1.9333 - accuracy: 0.2778 - val_loss: 1.9028 - val_accuracy: 0.2861 - lr: 9.9923e-04\n",
      "Epoch 75/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.9659 - accuracy: 0.2340 - val_loss: 1.9099 - val_accuracy: 0.2944 - lr: 9.9922e-04\n",
      "Epoch 76/10000\n",
      "12/12 [==============================] - 11s 868ms/step - loss: 1.9260 - accuracy: 0.2694 - val_loss: 1.8766 - val_accuracy: 0.2806 - lr: 9.9921e-04\n",
      "Epoch 77/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.9494 - accuracy: 0.2646 - val_loss: 1.8916 - val_accuracy: 0.3000 - lr: 9.9920e-04\n",
      "Epoch 78/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 1.9410 - accuracy: 0.2771 - val_loss: 1.8125 - val_accuracy: 0.3389 - lr: 9.9919e-04\n",
      "Epoch 79/10000\n",
      "12/12 [==============================] - 10s 841ms/step - loss: 1.9324 - accuracy: 0.2583 - val_loss: 1.8762 - val_accuracy: 0.2639 - lr: 9.9918e-04\n",
      "Epoch 80/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.8849 - accuracy: 0.2812 - val_loss: 1.8458 - val_accuracy: 0.2806 - lr: 9.9917e-04\n",
      "Epoch 81/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.8960 - accuracy: 0.2799 - val_loss: 1.7864 - val_accuracy: 0.3194 - lr: 9.9916e-04\n",
      "Epoch 82/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 1.8744 - accuracy: 0.2937 - val_loss: 1.8416 - val_accuracy: 0.2972 - lr: 9.9915e-04\n",
      "Epoch 83/10000\n",
      "12/12 [==============================] - 10s 850ms/step - loss: 1.8672 - accuracy: 0.2861 - val_loss: 1.9303 - val_accuracy: 0.2722 - lr: 9.9914e-04\n",
      "Epoch 84/10000\n",
      "12/12 [==============================] - 11s 867ms/step - loss: 1.8711 - accuracy: 0.2924 - val_loss: 1.8266 - val_accuracy: 0.3278 - lr: 9.9913e-04\n",
      "Epoch 85/10000\n",
      "12/12 [==============================] - 11s 854ms/step - loss: 1.8014 - accuracy: 0.2993 - val_loss: 1.8265 - val_accuracy: 0.2833 - lr: 9.9912e-04\n",
      "Epoch 86/10000\n",
      "12/12 [==============================] - 11s 870ms/step - loss: 1.8241 - accuracy: 0.2965 - val_loss: 1.7548 - val_accuracy: 0.3444 - lr: 9.9910e-04\n",
      "Epoch 87/10000\n",
      "12/12 [==============================] - 10s 848ms/step - loss: 1.8013 - accuracy: 0.3222 - val_loss: 1.7816 - val_accuracy: 0.3028 - lr: 9.9909e-04\n",
      "Epoch 88/10000\n",
      "12/12 [==============================] - 11s 855ms/step - loss: 1.8094 - accuracy: 0.3014 - val_loss: 1.7848 - val_accuracy: 0.3139 - lr: 9.9908e-04\n",
      "Epoch 89/10000\n",
      "12/12 [==============================] - 10s 834ms/step - loss: 1.8175 - accuracy: 0.3132 - val_loss: 1.7469 - val_accuracy: 0.3167 - lr: 9.9907e-04\n",
      "Epoch 90/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 1.7805 - accuracy: 0.3194 - val_loss: 1.7715 - val_accuracy: 0.3250 - lr: 9.9906e-04\n",
      "Epoch 91/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.7759 - accuracy: 0.3243 - val_loss: 1.7401 - val_accuracy: 0.3167 - lr: 9.9905e-04\n",
      "Epoch 92/10000\n",
      "12/12 [==============================] - 11s 867ms/step - loss: 1.7806 - accuracy: 0.3243 - val_loss: 1.7427 - val_accuracy: 0.3083 - lr: 9.9904e-04\n",
      "Epoch 93/10000\n",
      "12/12 [==============================] - 11s 868ms/step - loss: 1.7573 - accuracy: 0.3444 - val_loss: 1.7516 - val_accuracy: 0.3222 - lr: 9.9903e-04\n",
      "Epoch 94/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.7693 - accuracy: 0.3194 - val_loss: 1.7068 - val_accuracy: 0.3306 - lr: 9.9902e-04\n",
      "Epoch 95/10000\n",
      "12/12 [==============================] - 11s 878ms/step - loss: 1.7520 - accuracy: 0.3354 - val_loss: 1.8328 - val_accuracy: 0.3222 - lr: 9.9901e-04\n",
      "Epoch 96/10000\n",
      "12/12 [==============================] - 10s 846ms/step - loss: 1.7632 - accuracy: 0.3049 - val_loss: 1.7006 - val_accuracy: 0.3500 - lr: 9.9900e-04\n",
      "Epoch 97/10000\n",
      "12/12 [==============================] - 11s 876ms/step - loss: 1.7084 - accuracy: 0.3243 - val_loss: 1.7060 - val_accuracy: 0.3194 - lr: 9.9899e-04\n",
      "Epoch 98/10000\n",
      "12/12 [==============================] - 11s 897ms/step - loss: 1.6973 - accuracy: 0.3424 - val_loss: 1.6909 - val_accuracy: 0.3417 - lr: 9.9898e-04\n",
      "Epoch 99/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.7263 - accuracy: 0.3500 - val_loss: 1.7291 - val_accuracy: 0.3722 - lr: 9.9897e-04\n",
      "Epoch 100/10000\n",
      "12/12 [==============================] - 10s 847ms/step - loss: 1.6891 - accuracy: 0.3472 - val_loss: 1.8016 - val_accuracy: 0.3361 - lr: 9.9896e-04\n",
      "Epoch 101/10000\n",
      "12/12 [==============================] - 10s 854ms/step - loss: 1.7298 - accuracy: 0.3535 - val_loss: 1.6857 - val_accuracy: 0.3472 - lr: 9.9895e-04\n",
      "Epoch 102/10000\n",
      "12/12 [==============================] - 10s 846ms/step - loss: 1.7086 - accuracy: 0.3410 - val_loss: 1.6885 - val_accuracy: 0.3583 - lr: 9.9894e-04\n",
      "Epoch 103/10000\n",
      "12/12 [==============================] - 11s 862ms/step - loss: 1.6914 - accuracy: 0.3479 - val_loss: 1.6940 - val_accuracy: 0.3528 - lr: 9.9893e-04\n",
      "Epoch 104/10000\n",
      "12/12 [==============================] - 10s 851ms/step - loss: 1.6758 - accuracy: 0.3590 - val_loss: 1.6638 - val_accuracy: 0.3694 - lr: 9.9892e-04\n",
      "Epoch 105/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 1.7073 - accuracy: 0.3347 - val_loss: 1.6491 - val_accuracy: 0.3417 - lr: 9.9890e-04\n",
      "Epoch 106/10000\n",
      "12/12 [==============================] - 11s 857ms/step - loss: 1.6667 - accuracy: 0.3681 - val_loss: 1.7055 - val_accuracy: 0.3306 - lr: 9.9889e-04\n",
      "Epoch 107/10000\n",
      "12/12 [==============================] - 11s 879ms/step - loss: 1.6495 - accuracy: 0.3542 - val_loss: 1.6314 - val_accuracy: 0.3750 - lr: 9.9888e-04\n",
      "Epoch 108/10000\n",
      "12/12 [==============================] - 10s 846ms/step - loss: 1.6689 - accuracy: 0.3410 - val_loss: 1.6258 - val_accuracy: 0.3667 - lr: 9.9887e-04\n",
      "Epoch 109/10000\n",
      "12/12 [==============================] - 11s 878ms/step - loss: 1.6416 - accuracy: 0.3694 - val_loss: 1.6579 - val_accuracy: 0.3694 - lr: 9.9886e-04\n",
      "Epoch 110/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.6441 - accuracy: 0.3778 - val_loss: 1.7067 - val_accuracy: 0.3917 - lr: 9.9885e-04\n",
      "Epoch 111/10000\n",
      "12/12 [==============================] - 11s 876ms/step - loss: 1.6273 - accuracy: 0.3806 - val_loss: 1.6748 - val_accuracy: 0.3917 - lr: 9.9884e-04\n",
      "Epoch 112/10000\n",
      "12/12 [==============================] - 10s 843ms/step - loss: 1.6605 - accuracy: 0.3660 - val_loss: 1.6180 - val_accuracy: 0.3889 - lr: 9.9883e-04\n",
      "Epoch 113/10000\n",
      "12/12 [==============================] - 11s 867ms/step - loss: 1.6191 - accuracy: 0.3729 - val_loss: 1.6643 - val_accuracy: 0.3750 - lr: 9.9882e-04\n",
      "Epoch 114/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.6085 - accuracy: 0.3840 - val_loss: 1.6690 - val_accuracy: 0.3444 - lr: 9.9881e-04\n",
      "Epoch 115/10000\n",
      "12/12 [==============================] - 11s 884ms/step - loss: 1.5651 - accuracy: 0.3910 - val_loss: 1.6399 - val_accuracy: 0.4083 - lr: 9.9880e-04\n",
      "Epoch 116/10000\n",
      "12/12 [==============================] - 10s 839ms/step - loss: 1.6350 - accuracy: 0.3785 - val_loss: 1.6366 - val_accuracy: 0.3833 - lr: 9.9879e-04\n",
      "Epoch 117/10000\n",
      "12/12 [==============================] - 11s 861ms/step - loss: 1.6067 - accuracy: 0.3708 - val_loss: 1.6484 - val_accuracy: 0.3889 - lr: 9.9878e-04\n",
      "Epoch 118/10000\n",
      "12/12 [==============================] - 10s 839ms/step - loss: 1.5941 - accuracy: 0.4104 - val_loss: 1.6168 - val_accuracy: 0.3889 - lr: 9.9877e-04\n",
      "Epoch 119/10000\n",
      "12/12 [==============================] - 11s 849ms/step - loss: 1.5954 - accuracy: 0.3958 - val_loss: 1.6450 - val_accuracy: 0.4028 - lr: 9.9876e-04\n",
      "Epoch 120/10000\n",
      "12/12 [==============================] - 10s 841ms/step - loss: 1.6109 - accuracy: 0.3812 - val_loss: 1.5870 - val_accuracy: 0.3917 - lr: 9.9875e-04\n",
      "Epoch 121/10000\n",
      "12/12 [==============================] - 11s 862ms/step - loss: 1.5682 - accuracy: 0.3958 - val_loss: 1.6073 - val_accuracy: 0.3917 - lr: 9.9874e-04\n",
      "Epoch 122/10000\n",
      "12/12 [==============================] - 11s 863ms/step - loss: 1.5690 - accuracy: 0.3938 - val_loss: 1.6002 - val_accuracy: 0.4000 - lr: 9.9873e-04\n",
      "Epoch 123/10000\n",
      "12/12 [==============================] - 11s 848ms/step - loss: 1.5956 - accuracy: 0.3792 - val_loss: 1.6027 - val_accuracy: 0.3667 - lr: 9.9872e-04\n",
      "Epoch 124/10000\n",
      "12/12 [==============================] - 11s 859ms/step - loss: 1.5909 - accuracy: 0.4042 - val_loss: 1.5886 - val_accuracy: 0.3917 - lr: 9.9870e-04\n",
      "Epoch 125/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.5958 - accuracy: 0.3840 - val_loss: 1.5602 - val_accuracy: 0.4417 - lr: 9.9869e-04\n",
      "Epoch 126/10000\n",
      "12/12 [==============================] - 11s 866ms/step - loss: 1.5627 - accuracy: 0.4014 - val_loss: 1.5868 - val_accuracy: 0.4111 - lr: 9.9868e-04\n",
      "Epoch 127/10000\n",
      "12/12 [==============================] - 11s 865ms/step - loss: 1.5767 - accuracy: 0.3972 - val_loss: 1.5732 - val_accuracy: 0.3806 - lr: 9.9867e-04\n",
      "Epoch 128/10000\n",
      "12/12 [==============================] - 11s 898ms/step - loss: 1.5645 - accuracy: 0.3889 - val_loss: 1.5490 - val_accuracy: 0.4222 - lr: 9.9866e-04\n",
      "Epoch 129/10000\n",
      "12/12 [==============================] - 10s 852ms/step - loss: 1.5228 - accuracy: 0.4125 - val_loss: 1.5474 - val_accuracy: 0.4000 - lr: 9.9865e-04\n",
      "Epoch 130/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.4917 - accuracy: 0.4042 - val_loss: 1.5396 - val_accuracy: 0.4194 - lr: 9.9864e-04\n",
      "Epoch 131/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.5109 - accuracy: 0.4139 - val_loss: 1.5670 - val_accuracy: 0.4361 - lr: 9.9863e-04\n",
      "Epoch 132/10000\n",
      "12/12 [==============================] - 11s 867ms/step - loss: 1.5737 - accuracy: 0.3986 - val_loss: 1.5743 - val_accuracy: 0.3972 - lr: 9.9862e-04\n",
      "Epoch 133/10000\n",
      "12/12 [==============================] - 10s 831ms/step - loss: 1.5293 - accuracy: 0.4014 - val_loss: 1.5388 - val_accuracy: 0.4056 - lr: 9.9861e-04\n",
      "Epoch 134/10000\n",
      "12/12 [==============================] - 11s 866ms/step - loss: 1.5255 - accuracy: 0.4042 - val_loss: 1.6383 - val_accuracy: 0.3972 - lr: 9.9860e-04\n",
      "Epoch 135/10000\n",
      "12/12 [==============================] - 10s 841ms/step - loss: 1.5590 - accuracy: 0.4167 - val_loss: 1.5906 - val_accuracy: 0.4056 - lr: 9.9859e-04\n",
      "Epoch 136/10000\n",
      "12/12 [==============================] - 11s 864ms/step - loss: 1.5123 - accuracy: 0.4292 - val_loss: 1.5662 - val_accuracy: 0.4056 - lr: 9.9858e-04\n",
      "Epoch 137/10000\n",
      "12/12 [==============================] - 10s 842ms/step - loss: 1.5677 - accuracy: 0.4028 - val_loss: 1.5502 - val_accuracy: 0.3722 - lr: 9.9857e-04\n",
      "Epoch 138/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.5529 - accuracy: 0.4042 - val_loss: 1.5454 - val_accuracy: 0.4056 - lr: 9.9856e-04\n",
      "Epoch 139/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.4971 - accuracy: 0.4215 - val_loss: 1.5049 - val_accuracy: 0.4167 - lr: 9.9855e-04\n",
      "Epoch 140/10000\n",
      "12/12 [==============================] - 11s 882ms/step - loss: 1.4780 - accuracy: 0.4382 - val_loss: 1.5234 - val_accuracy: 0.4000 - lr: 9.9854e-04\n",
      "Epoch 141/10000\n",
      "12/12 [==============================] - 10s 837ms/step - loss: 1.5011 - accuracy: 0.4167 - val_loss: 1.5495 - val_accuracy: 0.4056 - lr: 9.9853e-04\n",
      "Epoch 142/10000\n",
      "12/12 [==============================] - 11s 882ms/step - loss: 1.4812 - accuracy: 0.4437 - val_loss: 1.5210 - val_accuracy: 0.4000 - lr: 9.9852e-04\n",
      "Epoch 143/10000\n",
      "12/12 [==============================] - 10s 842ms/step - loss: 1.4794 - accuracy: 0.4340 - val_loss: 1.5041 - val_accuracy: 0.4389 - lr: 9.9851e-04\n",
      "Epoch 144/10000\n",
      "12/12 [==============================] - 11s 856ms/step - loss: 1.4787 - accuracy: 0.4632 - val_loss: 1.5010 - val_accuracy: 0.4111 - lr: 9.9849e-04\n",
      "Epoch 145/10000\n",
      "12/12 [==============================] - 10s 827ms/step - loss: 1.4929 - accuracy: 0.4326 - val_loss: 1.5558 - val_accuracy: 0.3750 - lr: 9.9848e-04\n",
      "Epoch 146/10000\n",
      "12/12 [==============================] - 11s 867ms/step - loss: 1.4541 - accuracy: 0.4472 - val_loss: 1.5389 - val_accuracy: 0.4000 - lr: 9.9847e-04\n",
      "Epoch 147/10000\n",
      "12/12 [==============================] - 11s 864ms/step - loss: 1.4905 - accuracy: 0.4285 - val_loss: 1.4854 - val_accuracy: 0.4194 - lr: 9.9846e-04\n",
      "Epoch 148/10000\n",
      "12/12 [==============================] - 11s 847ms/step - loss: 1.4226 - accuracy: 0.4472 - val_loss: 1.5058 - val_accuracy: 0.4167 - lr: 9.9845e-04\n",
      "Epoch 149/10000\n",
      "12/12 [==============================] - 11s 863ms/step - loss: 1.4151 - accuracy: 0.4708 - val_loss: 1.5670 - val_accuracy: 0.4194 - lr: 9.9844e-04\n",
      "Epoch 150/10000\n",
      "12/12 [==============================] - 10s 844ms/step - loss: 1.4676 - accuracy: 0.4389 - val_loss: 1.4785 - val_accuracy: 0.4750 - lr: 9.9843e-04\n",
      "Epoch 151/10000\n",
      "12/12 [==============================] - 10s 851ms/step - loss: 1.4753 - accuracy: 0.4396 - val_loss: 1.4763 - val_accuracy: 0.4611 - lr: 9.9842e-04\n",
      "Epoch 152/10000\n",
      "12/12 [==============================] - 11s 855ms/step - loss: 1.4286 - accuracy: 0.4694 - val_loss: 1.5077 - val_accuracy: 0.4194 - lr: 9.9841e-04\n",
      "Epoch 153/10000\n",
      "12/12 [==============================] - 11s 874ms/step - loss: 1.4627 - accuracy: 0.4479 - val_loss: 1.4433 - val_accuracy: 0.4556 - lr: 9.9840e-04\n",
      "Epoch 154/10000\n",
      "12/12 [==============================] - 10s 850ms/step - loss: 1.4089 - accuracy: 0.4646 - val_loss: 1.4924 - val_accuracy: 0.4667 - lr: 9.9839e-04\n",
      "Epoch 155/10000\n",
      "12/12 [==============================] - 11s 866ms/step - loss: 1.4300 - accuracy: 0.4715 - val_loss: 1.4735 - val_accuracy: 0.4444 - lr: 9.9838e-04\n",
      "Epoch 156/10000\n",
      "12/12 [==============================] - 10s 839ms/step - loss: 1.4430 - accuracy: 0.4653 - val_loss: 1.4575 - val_accuracy: 0.4556 - lr: 9.9837e-04\n",
      "Epoch 157/10000\n",
      "12/12 [==============================] - 11s 909ms/step - loss: 1.4599 - accuracy: 0.4542 - val_loss: 1.4619 - val_accuracy: 0.4694 - lr: 9.9836e-04\n",
      "Epoch 158/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.3874 - accuracy: 0.4715 - val_loss: 1.4389 - val_accuracy: 0.4722 - lr: 9.9835e-04\n",
      "Epoch 159/10000\n",
      "12/12 [==============================] - 11s 880ms/step - loss: 1.4391 - accuracy: 0.4625 - val_loss: 1.4788 - val_accuracy: 0.4889 - lr: 9.9834e-04\n",
      "Epoch 160/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.4655 - accuracy: 0.4646 - val_loss: 1.4302 - val_accuracy: 0.4556 - lr: 9.9833e-04\n",
      "Epoch 161/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.4113 - accuracy: 0.4674 - val_loss: 1.5238 - val_accuracy: 0.4639 - lr: 9.9832e-04\n",
      "Epoch 162/10000\n",
      "12/12 [==============================] - 10s 844ms/step - loss: 1.4210 - accuracy: 0.4694 - val_loss: 1.4148 - val_accuracy: 0.4667 - lr: 9.9831e-04\n",
      "Epoch 163/10000\n",
      "12/12 [==============================] - 11s 865ms/step - loss: 1.4855 - accuracy: 0.4479 - val_loss: 1.4861 - val_accuracy: 0.4722 - lr: 9.9829e-04\n",
      "Epoch 164/10000\n",
      "12/12 [==============================] - 10s 843ms/step - loss: 1.4277 - accuracy: 0.4701 - val_loss: 1.4450 - val_accuracy: 0.4806 - lr: 9.9828e-04\n",
      "Epoch 165/10000\n",
      "12/12 [==============================] - 11s 872ms/step - loss: 1.4342 - accuracy: 0.4625 - val_loss: 1.4629 - val_accuracy: 0.4667 - lr: 9.9827e-04\n",
      "Epoch 166/10000\n",
      "12/12 [==============================] - 10s 844ms/step - loss: 1.4122 - accuracy: 0.4840 - val_loss: 1.4313 - val_accuracy: 0.4889 - lr: 9.9826e-04\n",
      "Epoch 167/10000\n",
      "12/12 [==============================] - 11s 866ms/step - loss: 1.3601 - accuracy: 0.4979 - val_loss: 1.3974 - val_accuracy: 0.4944 - lr: 9.9825e-04\n",
      "Epoch 168/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 1.3760 - accuracy: 0.4861 - val_loss: 1.4536 - val_accuracy: 0.4611 - lr: 9.9824e-04\n",
      "Epoch 169/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 1.3707 - accuracy: 0.4965 - val_loss: 1.3977 - val_accuracy: 0.4750 - lr: 9.9823e-04\n",
      "Epoch 170/10000\n",
      "12/12 [==============================] - 11s 857ms/step - loss: 1.3669 - accuracy: 0.4840 - val_loss: 1.4469 - val_accuracy: 0.4778 - lr: 9.9822e-04\n",
      "Epoch 171/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.3367 - accuracy: 0.4944 - val_loss: 1.4027 - val_accuracy: 0.4889 - lr: 9.9821e-04\n",
      "Epoch 172/10000\n",
      "12/12 [==============================] - 11s 853ms/step - loss: 1.3421 - accuracy: 0.4951 - val_loss: 1.4215 - val_accuracy: 0.4722 - lr: 9.9820e-04\n",
      "Epoch 173/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.3835 - accuracy: 0.4951 - val_loss: 1.3663 - val_accuracy: 0.4889 - lr: 9.9819e-04\n",
      "Epoch 174/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.3745 - accuracy: 0.4924 - val_loss: 1.4076 - val_accuracy: 0.4917 - lr: 9.9818e-04\n",
      "Epoch 175/10000\n",
      "12/12 [==============================] - 11s 867ms/step - loss: 1.3553 - accuracy: 0.4722 - val_loss: 1.3915 - val_accuracy: 0.4722 - lr: 9.9817e-04\n",
      "Epoch 176/10000\n",
      "12/12 [==============================] - 10s 849ms/step - loss: 1.3330 - accuracy: 0.5056 - val_loss: 1.3921 - val_accuracy: 0.4972 - lr: 9.9816e-04\n",
      "Epoch 177/10000\n",
      "12/12 [==============================] - 10s 851ms/step - loss: 1.3459 - accuracy: 0.5028 - val_loss: 1.3374 - val_accuracy: 0.5056 - lr: 9.9815e-04\n",
      "Epoch 178/10000\n",
      "12/12 [==============================] - 11s 884ms/step - loss: 1.3519 - accuracy: 0.4917 - val_loss: 1.4007 - val_accuracy: 0.4833 - lr: 9.9814e-04\n",
      "Epoch 179/10000\n",
      "12/12 [==============================] - 11s 856ms/step - loss: 1.3324 - accuracy: 0.5139 - val_loss: 1.3816 - val_accuracy: 0.5250 - lr: 9.9813e-04\n",
      "Epoch 180/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.3618 - accuracy: 0.4931 - val_loss: 1.3640 - val_accuracy: 0.5083 - lr: 9.9812e-04\n",
      "Epoch 181/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 1.3148 - accuracy: 0.4993 - val_loss: 1.3576 - val_accuracy: 0.5306 - lr: 9.9811e-04\n",
      "Epoch 182/10000\n",
      "12/12 [==============================] - 11s 859ms/step - loss: 1.2924 - accuracy: 0.5222 - val_loss: 1.3363 - val_accuracy: 0.4917 - lr: 9.9809e-04\n",
      "Epoch 183/10000\n",
      "12/12 [==============================] - 10s 843ms/step - loss: 1.3171 - accuracy: 0.5174 - val_loss: 1.4628 - val_accuracy: 0.4472 - lr: 9.9808e-04\n",
      "Epoch 184/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 1.3451 - accuracy: 0.4903 - val_loss: 1.3948 - val_accuracy: 0.4972 - lr: 9.9807e-04\n",
      "Epoch 185/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.2760 - accuracy: 0.5257 - val_loss: 1.4095 - val_accuracy: 0.5000 - lr: 9.9806e-04\n",
      "Epoch 186/10000\n",
      "12/12 [==============================] - 11s 908ms/step - loss: 1.3125 - accuracy: 0.5090 - val_loss: 1.4540 - val_accuracy: 0.4889 - lr: 9.9805e-04\n",
      "Epoch 187/10000\n",
      "12/12 [==============================] - 11s 852ms/step - loss: 1.2739 - accuracy: 0.5222 - val_loss: 1.3781 - val_accuracy: 0.5222 - lr: 9.9804e-04\n",
      "Epoch 188/10000\n",
      "12/12 [==============================] - 10s 854ms/step - loss: 1.2854 - accuracy: 0.5153 - val_loss: 1.3433 - val_accuracy: 0.4833 - lr: 9.9803e-04\n",
      "Epoch 189/10000\n",
      "12/12 [==============================] - 10s 838ms/step - loss: 1.2952 - accuracy: 0.5243 - val_loss: 1.3452 - val_accuracy: 0.5083 - lr: 9.9802e-04\n",
      "Epoch 190/10000\n",
      "12/12 [==============================] - 11s 870ms/step - loss: 1.3578 - accuracy: 0.5118 - val_loss: 1.3759 - val_accuracy: 0.5111 - lr: 9.9801e-04\n",
      "Epoch 191/10000\n",
      "12/12 [==============================] - 10s 834ms/step - loss: 1.2953 - accuracy: 0.5236 - val_loss: 1.4211 - val_accuracy: 0.4833 - lr: 9.9800e-04\n",
      "Epoch 192/10000\n",
      "12/12 [==============================] - 10s 854ms/step - loss: 1.3056 - accuracy: 0.5264 - val_loss: 1.4202 - val_accuracy: 0.4861 - lr: 9.9799e-04\n",
      "Epoch 193/10000\n",
      "12/12 [==============================] - 10s 844ms/step - loss: 1.3229 - accuracy: 0.5007 - val_loss: 1.3900 - val_accuracy: 0.4750 - lr: 9.9798e-04\n",
      "Epoch 194/10000\n",
      "12/12 [==============================] - 10s 849ms/step - loss: 1.2980 - accuracy: 0.5201 - val_loss: 1.3279 - val_accuracy: 0.5167 - lr: 9.9797e-04\n",
      "Epoch 195/10000\n",
      "12/12 [==============================] - 10s 847ms/step - loss: 1.2792 - accuracy: 0.5111 - val_loss: 1.3317 - val_accuracy: 0.4889 - lr: 9.9796e-04\n",
      "Epoch 196/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.2134 - accuracy: 0.5410 - val_loss: 1.3342 - val_accuracy: 0.5222 - lr: 9.9795e-04\n",
      "Epoch 197/10000\n",
      "12/12 [==============================] - 10s 844ms/step - loss: 1.2930 - accuracy: 0.5299 - val_loss: 1.3889 - val_accuracy: 0.4861 - lr: 9.9794e-04\n",
      "Epoch 198/10000\n",
      "12/12 [==============================] - 10s 855ms/step - loss: 1.2744 - accuracy: 0.5396 - val_loss: 1.2730 - val_accuracy: 0.5472 - lr: 9.9793e-04\n",
      "Epoch 199/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.2440 - accuracy: 0.5278 - val_loss: 1.3268 - val_accuracy: 0.5278 - lr: 9.9792e-04\n",
      "Epoch 200/10000\n",
      "12/12 [==============================] - 11s 865ms/step - loss: 1.2073 - accuracy: 0.5472 - val_loss: 1.2769 - val_accuracy: 0.5361 - lr: 9.9791e-04\n",
      "Epoch 201/10000\n",
      "12/12 [==============================] - 10s 839ms/step - loss: 1.2379 - accuracy: 0.5410 - val_loss: 1.3524 - val_accuracy: 0.5194 - lr: 9.9790e-04\n",
      "Epoch 202/10000\n",
      "12/12 [==============================] - 11s 857ms/step - loss: 1.3161 - accuracy: 0.5306 - val_loss: 1.3582 - val_accuracy: 0.5000 - lr: 9.9788e-04\n",
      "Epoch 203/10000\n",
      "12/12 [==============================] - 10s 858ms/step - loss: 1.2351 - accuracy: 0.5410 - val_loss: 1.3062 - val_accuracy: 0.5139 - lr: 9.9787e-04\n",
      "Epoch 204/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.3212 - accuracy: 0.5333 - val_loss: 1.3146 - val_accuracy: 0.5333 - lr: 9.9786e-04\n",
      "Epoch 205/10000\n",
      "12/12 [==============================] - 10s 846ms/step - loss: 1.2873 - accuracy: 0.5312 - val_loss: 1.2548 - val_accuracy: 0.5611 - lr: 9.9785e-04\n",
      "Epoch 206/10000\n",
      "12/12 [==============================] - 11s 864ms/step - loss: 1.2396 - accuracy: 0.5583 - val_loss: 1.3701 - val_accuracy: 0.5250 - lr: 9.9784e-04\n",
      "Epoch 207/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.2476 - accuracy: 0.5542 - val_loss: 1.2774 - val_accuracy: 0.5556 - lr: 9.9783e-04\n",
      "Epoch 208/10000\n",
      "12/12 [==============================] - 10s 852ms/step - loss: 1.2134 - accuracy: 0.5576 - val_loss: 1.2738 - val_accuracy: 0.5444 - lr: 9.9782e-04\n",
      "Epoch 209/10000\n",
      "12/12 [==============================] - 11s 864ms/step - loss: 1.2082 - accuracy: 0.5521 - val_loss: 1.2665 - val_accuracy: 0.5417 - lr: 9.9781e-04\n",
      "Epoch 210/10000\n",
      "12/12 [==============================] - 11s 854ms/step - loss: 1.2520 - accuracy: 0.5437 - val_loss: 1.2652 - val_accuracy: 0.5444 - lr: 9.9780e-04\n",
      "Epoch 211/10000\n",
      "12/12 [==============================] - 10s 846ms/step - loss: 1.2462 - accuracy: 0.5403 - val_loss: 1.2188 - val_accuracy: 0.5722 - lr: 9.9779e-04\n",
      "Epoch 212/10000\n",
      "12/12 [==============================] - 11s 859ms/step - loss: 1.1671 - accuracy: 0.5729 - val_loss: 1.3144 - val_accuracy: 0.4917 - lr: 9.9778e-04\n",
      "Epoch 213/10000\n",
      "12/12 [==============================] - 10s 855ms/step - loss: 1.1928 - accuracy: 0.5611 - val_loss: 1.3386 - val_accuracy: 0.5278 - lr: 9.9777e-04\n",
      "Epoch 214/10000\n",
      "12/12 [==============================] - 10s 841ms/step - loss: 1.2120 - accuracy: 0.5618 - val_loss: 1.2752 - val_accuracy: 0.5806 - lr: 9.9776e-04\n",
      "Epoch 215/10000\n",
      "12/12 [==============================] - 11s 884ms/step - loss: 1.2427 - accuracy: 0.5562 - val_loss: 1.2877 - val_accuracy: 0.5361 - lr: 9.9775e-04\n",
      "Epoch 216/10000\n",
      "12/12 [==============================] - 11s 895ms/step - loss: 1.1873 - accuracy: 0.5646 - val_loss: 1.2424 - val_accuracy: 0.5639 - lr: 9.9774e-04\n",
      "Epoch 217/10000\n",
      "12/12 [==============================] - 11s 868ms/step - loss: 1.1413 - accuracy: 0.5813 - val_loss: 1.3162 - val_accuracy: 0.5222 - lr: 9.9773e-04\n",
      "Epoch 218/10000\n",
      "12/12 [==============================] - 10s 850ms/step - loss: 1.2086 - accuracy: 0.5667 - val_loss: 1.2801 - val_accuracy: 0.5861 - lr: 9.9772e-04\n",
      "Epoch 219/10000\n",
      "12/12 [==============================] - 11s 860ms/step - loss: 1.1928 - accuracy: 0.5708 - val_loss: 1.2287 - val_accuracy: 0.5667 - lr: 9.9771e-04\n",
      "Epoch 220/10000\n",
      "12/12 [==============================] - 10s 843ms/step - loss: 1.1760 - accuracy: 0.5861 - val_loss: 1.2565 - val_accuracy: 0.5583 - lr: 9.9770e-04\n",
      "Epoch 221/10000\n",
      "12/12 [==============================] - 11s 871ms/step - loss: 1.1297 - accuracy: 0.5861 - val_loss: 1.2523 - val_accuracy: 0.5500 - lr: 9.9768e-04\n",
      "Epoch 222/10000\n",
      "12/12 [==============================] - 11s 855ms/step - loss: 1.1366 - accuracy: 0.5806 - val_loss: 1.2566 - val_accuracy: 0.5583 - lr: 9.9767e-04\n",
      "Epoch 223/10000\n",
      "12/12 [==============================] - 11s 857ms/step - loss: 1.1836 - accuracy: 0.5785 - val_loss: 1.2648 - val_accuracy: 0.5556 - lr: 9.9766e-04\n",
      "Epoch 224/10000\n",
      "12/12 [==============================] - 11s 861ms/step - loss: 1.1178 - accuracy: 0.6000 - val_loss: 1.2102 - val_accuracy: 0.5833 - lr: 9.9765e-04\n",
      "Epoch 225/10000\n",
      "12/12 [==============================] - 11s 857ms/step - loss: 1.1199 - accuracy: 0.5965 - val_loss: 1.2135 - val_accuracy: 0.5806 - lr: 9.9764e-04\n",
      "Epoch 226/10000\n",
      "12/12 [==============================] - 10s 837ms/step - loss: 1.1874 - accuracy: 0.5799 - val_loss: 1.2153 - val_accuracy: 0.5583 - lr: 9.9763e-04\n",
      "Epoch 227/10000\n",
      "12/12 [==============================] - 10s 854ms/step - loss: 1.1844 - accuracy: 0.5917 - val_loss: 1.1660 - val_accuracy: 0.5944 - lr: 9.9762e-04\n",
      "Epoch 228/10000\n",
      "12/12 [==============================] - 11s 907ms/step - loss: 1.1049 - accuracy: 0.5986 - val_loss: 1.2179 - val_accuracy: 0.5833 - lr: 9.9761e-04\n",
      "Epoch 229/10000\n",
      "12/12 [==============================] - 11s 878ms/step - loss: 1.1492 - accuracy: 0.5951 - val_loss: 1.1891 - val_accuracy: 0.5694 - lr: 9.9760e-04\n",
      "Epoch 230/10000\n",
      "12/12 [==============================] - 11s 856ms/step - loss: 1.1735 - accuracy: 0.5806 - val_loss: 1.1989 - val_accuracy: 0.5500 - lr: 9.9759e-04\n",
      "Epoch 231/10000\n",
      "12/12 [==============================] - 11s 861ms/step - loss: 1.1344 - accuracy: 0.5944 - val_loss: 1.1465 - val_accuracy: 0.5833 - lr: 9.9758e-04\n",
      "Epoch 232/10000\n",
      "12/12 [==============================] - 10s 844ms/step - loss: 1.1406 - accuracy: 0.6076 - val_loss: 1.2044 - val_accuracy: 0.5944 - lr: 9.9757e-04\n",
      "Epoch 233/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.1534 - accuracy: 0.5951 - val_loss: 1.1434 - val_accuracy: 0.6194 - lr: 9.9756e-04\n",
      "Epoch 234/10000\n",
      "12/12 [==============================] - 11s 860ms/step - loss: 1.1772 - accuracy: 0.6062 - val_loss: 1.1956 - val_accuracy: 0.5972 - lr: 9.9755e-04\n",
      "Epoch 235/10000\n",
      "12/12 [==============================] - 11s 857ms/step - loss: 1.1303 - accuracy: 0.5951 - val_loss: 1.1851 - val_accuracy: 0.5917 - lr: 9.9754e-04\n",
      "Epoch 236/10000\n",
      "12/12 [==============================] - 10s 838ms/step - loss: 1.1288 - accuracy: 0.6104 - val_loss: 1.2776 - val_accuracy: 0.6028 - lr: 9.9753e-04\n",
      "Epoch 237/10000\n",
      "12/12 [==============================] - 11s 868ms/step - loss: 1.1975 - accuracy: 0.5819 - val_loss: 1.3437 - val_accuracy: 0.5389 - lr: 9.9752e-04\n",
      "Epoch 238/10000\n",
      "12/12 [==============================] - 10s 846ms/step - loss: 1.2070 - accuracy: 0.5722 - val_loss: 1.2065 - val_accuracy: 0.6000 - lr: 9.9751e-04\n",
      "Epoch 239/10000\n",
      "12/12 [==============================] - 11s 890ms/step - loss: 1.1737 - accuracy: 0.6076 - val_loss: 1.2091 - val_accuracy: 0.5667 - lr: 9.9750e-04\n",
      "Epoch 240/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.1378 - accuracy: 0.5972 - val_loss: 1.1658 - val_accuracy: 0.5917 - lr: 9.9749e-04\n",
      "Epoch 241/10000\n",
      "12/12 [==============================] - 11s 858ms/step - loss: 1.1398 - accuracy: 0.5986 - val_loss: 1.2276 - val_accuracy: 0.5750 - lr: 9.9747e-04\n",
      "Epoch 242/10000\n",
      "12/12 [==============================] - 10s 840ms/step - loss: 1.1121 - accuracy: 0.6007 - val_loss: 1.1964 - val_accuracy: 0.5917 - lr: 9.9746e-04\n",
      "Epoch 243/10000\n",
      "12/12 [==============================] - 11s 861ms/step - loss: 1.0869 - accuracy: 0.6181 - val_loss: 1.1883 - val_accuracy: 0.5806 - lr: 9.9745e-04\n",
      "Epoch 244/10000\n",
      "12/12 [==============================] - 11s 916ms/step - loss: 1.0963 - accuracy: 0.6257 - val_loss: 1.1488 - val_accuracy: 0.6194 - lr: 9.9744e-04\n",
      "Epoch 245/10000\n",
      "12/12 [==============================] - 12s 918ms/step - loss: 1.0947 - accuracy: 0.6181 - val_loss: 1.1982 - val_accuracy: 0.6000 - lr: 9.9743e-04\n",
      "Epoch 246/10000\n",
      "12/12 [==============================] - 11s 887ms/step - loss: 1.0754 - accuracy: 0.6181 - val_loss: 1.1641 - val_accuracy: 0.6306 - lr: 9.9742e-04\n",
      "Epoch 247/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.0967 - accuracy: 0.6069 - val_loss: 1.1552 - val_accuracy: 0.6278 - lr: 9.9741e-04\n",
      "Epoch 248/10000\n",
      "12/12 [==============================] - 11s 861ms/step - loss: 1.1470 - accuracy: 0.5889 - val_loss: 1.1854 - val_accuracy: 0.5861 - lr: 9.9740e-04\n",
      "Epoch 249/10000\n",
      "12/12 [==============================] - 10s 845ms/step - loss: 1.1479 - accuracy: 0.6021 - val_loss: 1.2101 - val_accuracy: 0.5972 - lr: 9.9739e-04\n",
      "Epoch 250/10000\n",
      "12/12 [==============================] - 11s 859ms/step - loss: 1.1387 - accuracy: 0.5965 - val_loss: 1.1744 - val_accuracy: 0.5972 - lr: 9.9738e-04\n",
      "Epoch 251/10000\n",
      "12/12 [==============================] - 10s 843ms/step - loss: 1.0864 - accuracy: 0.6069 - val_loss: 1.1985 - val_accuracy: 0.5833 - lr: 9.9737e-04\n",
      "Epoch 252/10000\n",
      "12/12 [==============================] - 11s 863ms/step - loss: 1.0669 - accuracy: 0.6292 - val_loss: 1.1744 - val_accuracy: 0.6000 - lr: 9.9736e-04\n",
      "Epoch 253/10000\n",
      "12/12 [==============================] - 11s 863ms/step - loss: 1.0531 - accuracy: 0.6194 - val_loss: 1.1982 - val_accuracy: 0.6167 - lr: 9.9735e-04\n",
      "Epoch 254/10000\n",
      "12/12 [==============================] - 11s 870ms/step - loss: 1.0793 - accuracy: 0.6236 - val_loss: 1.1638 - val_accuracy: 0.6222 - lr: 9.9734e-04\n",
      "Epoch 255/10000\n",
      "12/12 [==============================] - 11s 863ms/step - loss: 1.1028 - accuracy: 0.6187 - val_loss: 1.1837 - val_accuracy: 0.6111 - lr: 9.9733e-04\n",
      "Epoch 256/10000\n",
      "12/12 [==============================] - 11s 873ms/step - loss: 1.0962 - accuracy: 0.6215 - val_loss: 1.2118 - val_accuracy: 0.6028 - lr: 9.9732e-04\n",
      "Epoch 257/10000\n",
      "12/12 [==============================] - 11s 872ms/step - loss: 1.1101 - accuracy: 0.6208 - val_loss: 1.1240 - val_accuracy: 0.6444 - lr: 9.9731e-04\n",
      "Epoch 258/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 1.1083 - accuracy: 0.6056 - val_loss: 1.1848 - val_accuracy: 0.6167 - lr: 9.9730e-04\n",
      "Epoch 259/10000\n",
      "12/12 [==============================] - 11s 877ms/step - loss: 1.0964 - accuracy: 0.6181 - val_loss: 1.2156 - val_accuracy: 0.6167 - lr: 9.9729e-04\n",
      "Epoch 260/10000\n",
      "12/12 [==============================] - 11s 874ms/step - loss: 1.0228 - accuracy: 0.6451 - val_loss: 1.2022 - val_accuracy: 0.6000 - lr: 9.9727e-04\n",
      "Epoch 261/10000\n",
      "12/12 [==============================] - 11s 884ms/step - loss: 1.0786 - accuracy: 0.6201 - val_loss: 1.1855 - val_accuracy: 0.5889 - lr: 9.9726e-04\n",
      "Epoch 262/10000\n",
      "12/12 [==============================] - 11s 866ms/step - loss: 1.0281 - accuracy: 0.6361 - val_loss: 1.2029 - val_accuracy: 0.6139 - lr: 9.9725e-04\n",
      "Epoch 263/10000\n",
      "12/12 [==============================] - 11s 869ms/step - loss: 1.0544 - accuracy: 0.6250 - val_loss: 1.1469 - val_accuracy: 0.6278 - lr: 9.9724e-04\n",
      "Epoch 264/10000\n",
      "12/12 [==============================] - 11s 863ms/step - loss: 1.0486 - accuracy: 0.6278 - val_loss: 1.1702 - val_accuracy: 0.6000 - lr: 9.9723e-04\n",
      "Epoch 265/10000\n",
      "12/12 [==============================] - 11s 897ms/step - loss: 1.0336 - accuracy: 0.6264 - val_loss: 1.0793 - val_accuracy: 0.6056 - lr: 9.9722e-04\n",
      "Epoch 266/10000\n",
      "12/12 [==============================] - 11s 874ms/step - loss: 0.9959 - accuracy: 0.6590 - val_loss: 1.1054 - val_accuracy: 0.6278 - lr: 9.9721e-04\n",
      "Epoch 267/10000\n",
      "12/12 [==============================] - 11s 874ms/step - loss: 1.0962 - accuracy: 0.6174 - val_loss: 1.1050 - val_accuracy: 0.6306 - lr: 9.9720e-04\n",
      "Epoch 268/10000\n",
      "12/12 [==============================] - 11s 891ms/step - loss: 1.0315 - accuracy: 0.6472 - val_loss: 1.2087 - val_accuracy: 0.6056 - lr: 9.9719e-04\n",
      "Epoch 269/10000\n",
      "12/12 [==============================] - 11s 902ms/step - loss: 1.0154 - accuracy: 0.6396 - val_loss: 1.1813 - val_accuracy: 0.6083 - lr: 9.9718e-04\n",
      "Epoch 270/10000\n",
      "12/12 [==============================] - 11s 919ms/step - loss: 1.0711 - accuracy: 0.6278 - val_loss: 1.1527 - val_accuracy: 0.6056 - lr: 9.9717e-04\n",
      "Epoch 271/10000\n",
      "12/12 [==============================] - 12s 1s/step - loss: 1.0015 - accuracy: 0.6667 - val_loss: 1.3023 - val_accuracy: 0.5806 - lr: 9.9716e-04\n",
      "Epoch 272/10000\n",
      "12/12 [==============================] - 12s 954ms/step - loss: 1.0978 - accuracy: 0.6306 - val_loss: 1.1184 - val_accuracy: 0.6278 - lr: 9.9715e-04\n",
      "Epoch 273/10000\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 1.0480 - accuracy: 0.6403 - val_loss: 1.1451 - val_accuracy: 0.6028 - lr: 9.9714e-04\n",
      "Epoch 274/10000\n",
      "12/12 [==============================] - 12s 986ms/step - loss: 1.0184 - accuracy: 0.6444 - val_loss: 1.0962 - val_accuracy: 0.6472 - lr: 9.9713e-04\n",
      "Epoch 275/10000\n",
      "12/12 [==============================] - 11s 898ms/step - loss: 1.0011 - accuracy: 0.6528 - val_loss: 1.1597 - val_accuracy: 0.6250 - lr: 9.9712e-04\n",
      "Epoch 276/10000\n",
      "12/12 [==============================] - 12s 942ms/step - loss: 1.0203 - accuracy: 0.6556 - val_loss: 1.1351 - val_accuracy: 0.6278 - lr: 9.9711e-04\n",
      "Epoch 277/10000\n",
      "12/12 [==============================] - 11s 911ms/step - loss: 1.0352 - accuracy: 0.6264 - val_loss: 1.1419 - val_accuracy: 0.6167 - lr: 9.9710e-04\n",
      "Epoch 278/10000\n",
      "12/12 [==============================] - 12s 951ms/step - loss: 0.9846 - accuracy: 0.6472 - val_loss: 1.1041 - val_accuracy: 0.6361 - lr: 9.9709e-04\n",
      "Epoch 279/10000\n",
      "12/12 [==============================] - 11s 869ms/step - loss: 1.0255 - accuracy: 0.6431 - val_loss: 1.1163 - val_accuracy: 0.6389 - lr: 9.9708e-04\n",
      "Epoch 280/10000\n",
      "12/12 [==============================] - 11s 884ms/step - loss: 1.0320 - accuracy: 0.6604 - val_loss: 1.1004 - val_accuracy: 0.6361 - lr: 9.9706e-04\n",
      "Epoch 281/10000\n",
      "12/12 [==============================] - 11s 853ms/step - loss: 0.9977 - accuracy: 0.6576 - val_loss: 1.1868 - val_accuracy: 0.6167 - lr: 9.9705e-04\n",
      "Epoch 282/10000\n",
      "12/12 [==============================] - 11s 881ms/step - loss: 1.0066 - accuracy: 0.6472 - val_loss: 1.1219 - val_accuracy: 0.6167 - lr: 9.9704e-04\n",
      "Epoch 283/10000\n",
      "12/12 [==============================] - 11s 876ms/step - loss: 0.9786 - accuracy: 0.6576 - val_loss: 1.1712 - val_accuracy: 0.6500 - lr: 9.9703e-04\n",
      "Epoch 284/10000\n",
      "12/12 [==============================] - 11s 876ms/step - loss: 0.9733 - accuracy: 0.6583 - val_loss: 1.0939 - val_accuracy: 0.6250 - lr: 9.9702e-04\n",
      "Epoch 285/10000\n",
      "12/12 [==============================] - 10s 854ms/step - loss: 0.9929 - accuracy: 0.6500 - val_loss: 1.0865 - val_accuracy: 0.6528 - lr: 9.9701e-04\n",
      "Epoch 286/10000\n",
      "12/12 [==============================] - 11s 878ms/step - loss: 0.9657 - accuracy: 0.6639 - val_loss: 1.0593 - val_accuracy: 0.6556 - lr: 9.9700e-04\n",
      "Epoch 287/10000\n",
      "12/12 [==============================] - 11s 874ms/step - loss: 0.9808 - accuracy: 0.6646 - val_loss: 1.1194 - val_accuracy: 0.6444 - lr: 9.9699e-04\n",
      "Epoch 288/10000\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "model.fit(train,\n",
    "  epochs = epochs,\n",
    "  validation_data = test,\n",
    "  callbacks= [\n",
    "    callbacks.EarlyStopping(\n",
    "      monitor = 'val_loss',\n",
    "      patience = patience,\n",
    "      verbose = 1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "      filepath = model_path,\n",
    "      save_weights_only = False,\n",
    "      monitor = 'loss',\n",
    "      mode = 'min',\n",
    "      save_best_only = True\n",
    "    ),\n",
    "    callbacks.LearningRateScheduler(\n",
    "      lr_schedule \n",
    "    )\n",
    "  ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
